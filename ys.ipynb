{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[Based on A novel architecture for web-based attack detection using convolutional neural network](https://www.sciencedirect.com/science/article/pii/S0167404820303692?via%3Dihub)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/640/1*ulfFYH5HbWpLTIfuebj5mQ.gif\" width = 400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">## PREPROCESSING\n",
    " - __The preprocessing consists of two steps:__ \n",
    "   1. The production of dictionaries\n",
    "   2. The production of matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('normalTrafficTraining.txt',sep=\"/n/n\", header= None)\n",
    "\n",
    "print(\"Shape:\", data.shape)\n",
    "print(\"\\nRecords Example:\\n\", data.head()) \n",
    "print(\"\\nInfo:\\n\", data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__`The header parts of the HTTP requests are removed. Since the header \n",
    "portions are generally the same in the HTTP requests dataset, they reduce the size of the data \n",
    "due to reduction of unnecessary data and eliminate the duplication of the data in datasets. The \n",
    "URL and payload portions of the HTTP request in datasets are selected`__\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whData = data[data[0].str.contains('GET|POST') \n",
    "              | (data[0].str.contains(': ') == False)]\n",
    "\n",
    "whData.loc[:, 0] = whData[0].apply(lambda x: \n",
    "    x.replace(\"GET \", \"\").replace(\"POST \", \"\").replace(\" HTTP/1.1\", \"\"))\n",
    "\n",
    "whData.loc[:, 0] = whData[0].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(whData.shape)\n",
    "print(whData.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = whData[0].str.split(\"&\",\n",
    "                              expand=True).stack().reset_index(level=1,\n",
    "                                                               drop=True)\n",
    "payload = whData[0].str.split(\"?\",\n",
    "                              expand=True).stack().reset_index(level=1,\n",
    "                                                               drop=True)\n",
    "payload.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Numpy arrays. For further investigation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "npdata = data.to_numpy()\n",
    "\n",
    "for i in range(0,20):\n",
    "    print(npdata[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npwhdata = whData.to_numpy()\n",
    "for i in range(0,10):\n",
    "    print(npwhdata[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npPayload = payload.to_numpy()\n",
    "for i in range(0,10):\n",
    "    print(npPayload[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### **Results**\n",
    "`Based on normalTrafficTraining.txt we've got the following`\n",
    "-  __Dataframes:__\n",
    "    *  data         _(source data)_\n",
    "    *  whData       _(contains URL and Payload)_\n",
    "    *  __payload__      _(Splitted Payload)_\n",
    ">\n",
    "- __Numpy Arrays:__\n",
    "    *  npdata       _(source data)_\n",
    "    *  npwhdata     _(contains URL and Payload)_\n",
    "    *  __npPayload__    _(Splitted Payload)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Dictionary production scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dictionary production.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some raw data containing URLs and payloads (variables and values)\n",
    "data = [\n",
    "    {\"url\": \"http://example.com/page1?var1=val1&var2=val2\", \"payloads\": [\"var1=val1\", \"var2=val2\"]},\n",
    "    {\"url\": \"http://example.com/page2?var1=val1&var2=val2\", \"payloads\": [\"var1=val1\", \"var2=val2\"]},\n",
    "    {\"url\": \"http://example.com/page3?var1=val1&var2=val3\", \"payloads\": [\"var1=val1\", \"var2=val3\"]},\n",
    "]\n",
    "\n",
    "# Create dictionaries from the raw data\n",
    "payload_dict = {}\n",
    "for dat in data:\n",
    "    # Add the payloads to the payload dictionary\n",
    "    for payload in dat[\"payloads\"]:\n",
    "        if payload not in payload_dict:\n",
    "            payload_dict[payload] = 1\n",
    "        else:\n",
    "            payload_dict[payload] += 1\n",
    "\n",
    "# Print the dictionaries\n",
    "print(payload_dict)\n",
    "\n",
    "# Use the Bag-of-Words technique to create matrixes from the dictionaries\n",
    "payload_matrix = []\n",
    "for dat in data:\n",
    "    # Create a BoW vector for the payloads\n",
    "    payload_vector = [0] * len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some raw data containing URLs and payloads\n",
    "data = [\n",
    "    {\"url\": \"http://example.com/page1?var1=val1&var2=val2\", \"payload\": \"Hello world!\"},\n",
    "    {\"url\": \"http://example.com/page2?var1=val1&var2=val2\", \"payload\": \"Hello world!\"},\n",
    "    {\"url\": \"http://example.com/page3?var1=val1&var2=val2\", \"payload\": \"Goodbye world!\"},\n",
    "]\n",
    "\n",
    "# Create dictionaries from the raw data\n",
    "url_dict = {}\n",
    "payload_dict = {}\n",
    "for dat in data:\n",
    "    # Split the URL into variables and values using the filter\n",
    "    url_parts = dat[\"url\"].split(\"?\")[1].split(\"&\")\n",
    "    for part in url_parts:\n",
    "        variable, value = part.split(\"=\")\n",
    "        if variable not in url_dict:\n",
    "            url_dict[variable] = [value]\n",
    "        else:\n",
    "            url_dict[variable].append(value)\n",
    "    # Add the payload to the payload dictionary\n",
    "    if dat[\"payload\"] not in payload_dict:\n",
    "        payload_dict[dat[\"payload\"]] = 1\n",
    "    else:\n",
    "        payload_dict[dat[\"payload\"]] += 1\n",
    "\n",
    "# Print the dictionaries\n",
    "print(url_dict)\n",
    "print(payload_dict)\n",
    "\n",
    "# Use the Bag-of-Words technique to create matrixes from the dictionaries\n",
    "url_matrix = []\n",
    "payload_matrix = []\n",
    "for dat in data:\n",
    "    # Create a BoW vector for the URL\n",
    "    url_vector = [0] * len(url_dict)\n",
    "    url_parts = dat[\"url\"].split(\"?\")[1].split(\"&\")\n",
    "    for part in url_parts:\n",
    "        variable, value = part.split(\"=\")\n",
    "        if variable in url_dict:\n",
    "            url_vector[url_dict[variable].index(value)] += 1\n",
    "    url_matrix.append(url_vector)\n",
    "    # Create a BoW vector for the payload\n",
    "    payload_vector = [0] * len(payload_dict)\n",
    "    if dat[\"payload\"] in payload_dict:\n",
    "        payload_vector[list(payload_dict.keys()).index(dat[\"payload\"])] += 1\n",
    "    payload_matrix.append(payload_vector)\n",
    "\n",
    "# Print the matrixes\n",
    "print(url_matrix)\n",
    "print(payload_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cool tools)\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demonstration of tokenization\n",
    "txt = 'id=3&nombre=Vino+Rioja&precio=100&cantidad=55&B1=A%F1adir+al+carrito'\n",
    "nltk.tokenize.regexp_tokenize(txt, pattern=r\"&\", gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demonstration no.2\n",
    "dataset = ['Game of Thrones is an amazing tv series!',\n",
    "'Game of Thrones is the best tv series!',\n",
    "'Game of Thrones is so great']\n",
    "\n",
    "word2count = {}\n",
    "for data1 in dataset:\n",
    "    words = nltk.tokenize.regexp_tokenize(data1, pattern=r\"\\s\", gaps=True)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "\n",
    "print(word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do procesing only with payloads (without URLs) because I still have no idea why we have URL here))\n",
    "payload_v2 = data[data[0].str.contains('GET|POST') \n",
    "              | (data[0].str.contains(': ') == False)]\n",
    "payload_v2 = payload_v2[payload_v2[0].str.contains('http') == False]\n",
    "\n",
    "dict = {}\n",
    "for rdata in payload_v2[0]:\n",
    "    words = nltk.tokenize.regexp_tokenize(rdata, pattern=r\"&\", gaps=True)\n",
    "    for word in words:\n",
    "        if word not in dict.keys():\n",
    "            dict[word] = 1\n",
    "        else:\n",
    "            dict[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try at your own risk\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd97b8bffa4d3737e84826bc3d37be3046061822757ce35137ab82ad4c5a2016"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
